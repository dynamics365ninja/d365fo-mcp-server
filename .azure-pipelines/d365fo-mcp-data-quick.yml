# Simplified Azure Pipeline for Daily Custom Metadata Updates
# This pipeline is optimized for quick updates of custom models only
# Triggers automatically on changes to main branch (no periodic runs)

trigger:
  branches:
    include:
      - main
  paths:
    include:
      - 'src/**'
      - '.azure-pipelines/d365fo-mcp-data-quick.yml'

# Note: No scheduled runs - pipeline runs only when changes are detected
# To force a run, use manual trigger with parameters below

# Manual trigger with parameters
parameters:
  - name: extractionMode
    displayName: 'Extraction Mode'
    type: string
    default: 'custom'
    values:
      - custom
      - standard
      - all
  - name: customModels
    displayName: 'Custom Models (comma-separated, leave empty for all)'
    type: string
    default: 'all'

pool:
  vmImage: 'ubuntu-latest'

resources:
  repositories:
    - repository: mcpServer
      type: github
      endpoint: github.com_dynamics365ninja
      name: dynamics365ninja/d365fo-mcp-server
      ref: refs/heads/main

variables:
  - group: xpp-mcp-server-config
  - name: nodeVersion
    value: '22.x'
  - name: METADATA_PATH
    value: './metadata'
  - name: DB_PATH
    value: './database/xpp-metadata.db'

stages:
  # ============================================================================
  # Single Stage: Process Metadata Based on Mode
  # ============================================================================
  - stage: ProcessMetadata
    displayName: 'Process Metadata'
    jobs:
      - job: ProcessJob
        displayName: 'Extract, Build, and Upload'
        steps:
          # ========== Checkout MCP Server code from GitHub ==========
          - checkout: mcpServer
            displayName: 'Checkout MCP Server code from GitHub'
            clean: true

          - task: NodeTool@0
            displayName: 'Install Node.js'
            inputs:
              versionSpec: $(nodeVersion)

          - script: npm ci
            displayName: 'Install dependencies'
            workingDirectory: '$(Build.SourcesDirectory)'

          # ========== Custom Mode Steps (default) ==========
          # In custom mode, we only update custom models while preserving standard models in the database
          
          # Download existing database
          - script: npm run blob-manager download-database
            displayName: 'Download existing database'
            condition: eq('${{ parameters.extractionMode }}', 'custom')
            workingDirectory: '$(Build.SourcesDirectory)'
            env:
              AZURE_STORAGE_CONNECTION_STRING: $(AZURE_STORAGE_CONNECTION_STRING)
              BLOB_CONTAINER_NAME: $(BLOB_CONTAINER_NAME)
              DB_PATH: $(DB_PATH)

          # Download standard metadata (cached)
          - script: npm run blob-manager download-standard
            displayName: 'Download standard metadata (cached)'
            condition: eq('${{ parameters.extractionMode }}', 'custom')
            workingDirectory: '$(Build.SourcesDirectory)'
            env:
              AZURE_STORAGE_CONNECTION_STRING: $(AZURE_STORAGE_CONNECTION_STRING)
              BLOB_CONTAINER_NAME: $(BLOB_CONTAINER_NAME)
              METADATA_PATH: $(METADATA_PATH)

          # Delete old custom metadata from blob
          - script: |
              if [ -n "${{ parameters.customModels }}" ] && [ "${{ parameters.customModels }}" != "all" ]; then
                npm run blob-manager delete-custom "${{ parameters.customModels }}"
              else
                npm run blob-manager delete-custom
              fi
            displayName: 'Delete old custom metadata from blob'
            condition: eq('${{ parameters.extractionMode }}', 'custom')
            workingDirectory: '$(Build.SourcesDirectory)'
            env:
              AZURE_STORAGE_CONNECTION_STRING: $(AZURE_STORAGE_CONNECTION_STRING)
              BLOB_CONTAINER_NAME: $(BLOB_CONTAINER_NAME)

          # Extract custom models from Git source
          - script: |
              if [ "${{ parameters.customModels }}" != "all" ]; then
                export CUSTOM_MODELS="${{ parameters.customModels }}"
              fi
              npm run extract-metadata
            displayName: 'Extract custom models from Git source'
            condition: eq('${{ parameters.extractionMode }}', 'custom')
            workingDirectory: '$(Build.SourcesDirectory)'
            env:
              PACKAGES_PATH: $(Pipeline.Workspace)/d365fo-source
              METADATA_PATH: $(METADATA_PATH)
              EXTRACT_MODE: 'custom'
              EXTENSION_PREFIX: $(EXTENSION_PREFIX)

          # ========== Full Rebuild Mode Steps ==========
          
          # Clean everything for full rebuild
          - script: |
              rm -rf $(METADATA_PATH)
              mkdir -p $(METADATA_PATH)
            displayName: 'Clean metadata directory'
            condition: ne('${{ parameters.extractionMode }}', 'custom')
            workingDirectory: '$(Build.SourcesDirectory)'

          # Extract all or specific mode
          - script: npm run extract-metadata
            displayName: 'Extract all metadata'
            condition: ne('${{ parameters.extractionMode }}', 'custom')
            workingDirectory: '$(Build.SourcesDirectory)'
            env:
              PACKAGES_PATH: $(Pipeline.Workspace)/d365fo-source
              METADATA_PATH: $(METADATA_PATH)
              EXTRACT_MODE: '${{ parameters.extractionMode }}'
              CUSTOM_MODELS: $(CUSTOM_MODELS)

          # ========== Common Steps for All Modes ==========
          
          # Build database (custom mode: only replaces custom models, standard mode: replaces standard models, all mode: full rebuild)
          - script: npm run build-database
            displayName: 'Build database'
            workingDirectory: '$(Build.SourcesDirectory)'
            env:
              METADATA_PATH: $(METADATA_PATH)
              DB_PATH: $(DB_PATH)

          # Upload metadata based on mode
          - script: |
              if [ "${{ parameters.customModels }}" != "all" ]; then
                export CUSTOM_MODELS="${{ parameters.customModels }}"
              fi
              npm run blob-manager upload-custom
            displayName: 'Upload custom metadata'
            condition: eq('${{ parameters.extractionMode }}', 'custom')
            workingDirectory: '$(Build.SourcesDirectory)'
            env:
              AZURE_STORAGE_CONNECTION_STRING: $(AZURE_STORAGE_CONNECTION_STRING)
              BLOB_CONTAINER_NAME: $(BLOB_CONTAINER_NAME)
              METADATA_PATH: $(METADATA_PATH)

          - script: |
              if [ "${{ parameters.extractionMode }}" = "standard" ]; then
                npm run blob-manager upload-standard
              elif [ "${{ parameters.extractionMode }}" = "all" ]; then
                npm run blob-manager upload-all
              fi
            displayName: 'Upload metadata to blob'
            condition: ne('${{ parameters.extractionMode }}', 'custom')
            workingDirectory: '$(Build.SourcesDirectory)'
            env:
              AZURE_STORAGE_CONNECTION_STRING: $(AZURE_STORAGE_CONNECTION_STRING)
              BLOB_CONTAINER_NAME: $(BLOB_CONTAINER_NAME)
              METADATA_PATH: $(METADATA_PATH)

          # Upload database
          - script: npm run blob-manager upload-database $(DB_PATH)
            displayName: 'Upload database'
            workingDirectory: '$(Build.SourcesDirectory)'
            env:
              AZURE_STORAGE_CONNECTION_STRING: $(AZURE_STORAGE_CONNECTION_STRING)
              BLOB_CONTAINER_NAME: $(BLOB_CONTAINER_NAME)

          # Restart App Service
          - task: AzureAppServiceManage@0
            displayName: 'Restart App Service'
            inputs:
              azureSubscription: '$(AZURE_SUBSCRIPTION)'
              action: 'Restart Azure App Service'
              webAppName: '$(AZURE_APP_SERVICE_NAME)'
